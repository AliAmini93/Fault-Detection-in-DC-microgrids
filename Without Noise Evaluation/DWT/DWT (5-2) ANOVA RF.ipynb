{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9PJKdBe8Obw","executionInfo":{"status":"ok","timestamp":1691179699459,"user_tz":-210,"elapsed":22841,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"fb3fbb45-f09d-4c2d-eac9-6485dc461b29"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eH14YbVJF2jJ","executionInfo":{"status":"ok","timestamp":1691179700093,"user_tz":-210,"elapsed":641,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"58dbb921-4baa-4110-b8fd-1d1017b4fa16"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Ali Sobhani Thesis/DWT\n"]}],"source":["cd '/content/drive/MyDrive/Ali Sobhani Thesis/DWT'"]},{"cell_type":"code","source":["'''import numpy as np\n","from sklearn.experimental import enable_halving_search_cv  # noqa\n","from sklearn.model_selection import HalvingGridSearchCV, StratifiedKFold\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import fbeta_score\n","from scipy.stats import pearsonr\n","\n","def pearsonr_scorer(X, y):\n","    scores, pvalues = [], []\n","    for column in X.T:\n","        score, pvalue = pearsonr(column, y)\n","        scores.append(abs(score))\n","        pvalues.append(pvalue)\n","    return np.array(scores), np.array(pvalues)\n","\n","feature_selection = SelectKBest(score_func=pearsonr_scorer)\n","feature_names = ['IP1_C1', 'IP1_C2', 'IP1_C3','IP1_C4','IN1_C1', 'IN1_C2', 'IN1_C3', 'IN1_C4',\n","                    'IP2_C1', 'IP2_C2', 'IP2_C3', 'IP2_C4', 'IN2_C1', 'IN2_C2', 'IN2_C3', 'IN2_C4',\n","                    'VP1_C1', 'VP1_C2', 'VP1_C3', 'VP1_C4', 'VN1_C1', 'VN1_C2', 'VN1_C3', 'VN1_C4',\n","                    'VP2_C1', 'VP2_C2', 'VP2_C3', 'VP2_C4', 'VN2_C1', 'VN2_C2', 'VN2_C3', 'VN2_C4']\n","\n","X = np.load('X.npy')\n","Y = np.load('Y.npy')\n","\n","rf = RandomForestClassifier()\n","\n","pipeline = Pipeline([\n","    ('feature_selection', feature_selection),\n","    ('rf', rf)\n","])\n","\n","param_grid = {\n","    'feature_selection__k': list(range(1, X.shape[1] + 1)),\n","    'rf__n_estimators': [10, 50, 100, 200, 500],\n","    'rf__max_depth': [None, 10, 20, 30, 40, 50],\n","    'rf__min_samples_split': [2, 5, 10],\n","    'rf__min_samples_leaf': [1, 2, 4],\n","    'rf__bootstrap': [True, False]\n","}\n","\n","outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n","inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\n","\n","grid_search = HalvingGridSearchCV(pipeline, param_grid, cv=inner_cv, scoring='accuracy')\n","\n","best_scores = {'accuracy': [], 'f2': []}\n","best_params = []\n","all_selected_names = []\n","\n","for train_idx, test_idx in outer_cv.split(X, Y):\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = Y[train_idx], Y[test_idx]\n","\n","    grid_search.fit(X_train, y_train)\n","    best_params.append(grid_search.best_params_)\n","\n","    best_scores['accuracy'].append(grid_search.best_score_)\n","\n","    y_pred = grid_search.predict(X_test)\n","\n","    f2 = fbeta_score(y_test, y_pred, beta=2)\n","    best_scores['f2'].append(f2)\n","\n","    selected_features = grid_search.best_estimator_.named_steps['feature_selection']\n","    selected_names = [feature_names[i] for i in selected_features.get_support(indices=True)]\n","    all_selected_names.append(selected_names)\n","\n","average_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\n","print(f\"Average best scores: {average_best_scores}\")\n","'''"],"metadata":{"id":"WYiiMD9oGuV0","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"d076bab3-cd77-4e97-f557-da830987c44b","executionInfo":{"status":"ok","timestamp":1691179700094,"user_tz":-210,"elapsed":10,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import numpy as np\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingGridSearchCV, StratifiedKFold\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import fbeta_score\\nfrom scipy.stats import pearsonr\\n\\ndef pearsonr_scorer(X, y):\\n    scores, pvalues = [], []\\n    for column in X.T:\\n        score, pvalue = pearsonr(column, y)\\n        scores.append(abs(score))\\n        pvalues.append(pvalue)\\n    return np.array(scores), np.array(pvalues)\\n\\nfeature_selection = SelectKBest(score_func=pearsonr_scorer)\\nfeature_names = [\\'IP1_C1\\', \\'IP1_C2\\', \\'IP1_C3\\',\\'IP1_C4\\',\\'IN1_C1\\', \\'IN1_C2\\', \\'IN1_C3\\', \\'IN1_C4\\',\\n                    \\'IP2_C1\\', \\'IP2_C2\\', \\'IP2_C3\\', \\'IP2_C4\\', \\'IN2_C1\\', \\'IN2_C2\\', \\'IN2_C3\\', \\'IN2_C4\\',\\n                    \\'VP1_C1\\', \\'VP1_C2\\', \\'VP1_C3\\', \\'VP1_C4\\', \\'VN1_C1\\', \\'VN1_C2\\', \\'VN1_C3\\', \\'VN1_C4\\',\\n                    \\'VP2_C1\\', \\'VP2_C2\\', \\'VP2_C3\\', \\'VP2_C4\\', \\'VN2_C1\\', \\'VN2_C2\\', \\'VN2_C3\\', \\'VN2_C4\\']\\n\\nX = np.load(\\'X.npy\\')\\nY = np.load(\\'Y.npy\\')\\n\\nrf = RandomForestClassifier()\\n\\npipeline = Pipeline([\\n    (\\'feature_selection\\', feature_selection),\\n    (\\'rf\\', rf)\\n])\\n\\nparam_grid = {\\n    \\'feature_selection__k\\': list(range(1, X.shape[1] + 1)),\\n    \\'rf__n_estimators\\': [10, 50, 100, 200, 500],\\n    \\'rf__max_depth\\': [None, 10, 20, 30, 40, 50],\\n    \\'rf__min_samples_split\\': [2, 5, 10],\\n    \\'rf__min_samples_leaf\\': [1, 2, 4],\\n    \\'rf__bootstrap\\': [True, False]\\n}\\n\\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\\ninner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\\n\\ngrid_search = HalvingGridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=\\'accuracy\\')\\n\\nbest_scores = {\\'accuracy\\': [], \\'f2\\': []}\\nbest_params = []\\nall_selected_names = []\\n\\nfor train_idx, test_idx in outer_cv.split(X, Y):\\n    X_train, X_test = X[train_idx], X[test_idx]\\n    y_train, y_test = Y[train_idx], Y[test_idx]\\n\\n    grid_search.fit(X_train, y_train)\\n    best_params.append(grid_search.best_params_)\\n\\n    best_scores[\\'accuracy\\'].append(grid_search.best_score_)\\n\\n    y_pred = grid_search.predict(X_test)\\n\\n    f2 = fbeta_score(y_test, y_pred, beta=2)\\n    best_scores[\\'f2\\'].append(f2)\\n\\n    selected_features = grid_search.best_estimator_.named_steps[\\'feature_selection\\']\\n    selected_names = [feature_names[i] for i in selected_features.get_support(indices=True)]\\n    all_selected_names.append(selected_names)\\n\\naverage_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\\nprint(f\"Average best scores: {average_best_scores}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.experimental import enable_halving_search_cv  # noqa\n","from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import fbeta_score\n","\n","feature_selection = SelectKBest(score_func=f_classif)\n","feature_names = ['IP1_E1', 'IP1_E2', 'IN1_E1','IN1_E2','IP2_E1', 'IP2_E2', 'IN2_E1', 'IN2_E2',\n","                    'VP1_E1', 'VP1_E2', 'VN1_E1', 'VN1_E2', 'VP2_E1', 'VP2_E2', 'VN2_E1', 'VN2_E2']\n","X = np.load('X.npy')\n","Y = np.load('Y.npy')\n","\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","pipeline = Pipeline([\n","    ('feature_selection', feature_selection),\n","    ('rf', rf)\n","])\n","\n","param_grid = {\n","    'feature_selection__k': list(range(1, X.shape[1] + 1)),\n","    'rf__n_estimators': [100, 200, 300],\n","    'rf__max_depth': [None, 30, 50],\n","    'rf__min_samples_split': [2, 5, 10],\n","    'rf__min_samples_leaf': [1, 2, 4],\n","    'rf__bootstrap': [True, False]\n","}\n","\n","outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n","inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\n","\n","grid_search = RandomizedSearchCV(pipeline, param_grid, n_iter=100, cv=inner_cv, scoring='accuracy', random_state=13, n_jobs=-1)\n","\n","best_scores = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_params = []\n","all_selected_names = []\n","\n","for train_idx, test_idx in outer_cv.split(X, Y):\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = Y[train_idx], Y[test_idx]\n","\n","    grid_search.fit(X_train, y_train)\n","    best_params.append(grid_search.best_params_)\n","\n","    #best_scores['accuracy'].append(grid_search.best_score_)\n","\n","    y_pred = grid_search.predict(X_test)\n","\n","    f2 = fbeta_score(y_test, y_pred, beta=2)\n","    ################################\n","    from sklearn.metrics import f1_score, accuracy_score\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    ################################\n","    best_scores['f2'].append(f2)\n","    best_scores['f1'].append(f1)\n","    best_scores['accuracy'].append(accuracy)\n","\n","    selected_features = grid_search.best_estimator_.named_steps['feature_selection']\n","    selected_names = [feature_names[i] for i in selected_features.get_support(indices=True)]\n","    all_selected_names.append(selected_names)\n","\n","average_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\n","print(f\"Average best scores: {average_best_scores}\")\n"],"metadata":{"id":"HANE6y7ngc8G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691180222102,"user_tz":-210,"elapsed":522014,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"7d95f46e-f67a-4d77-bd13-6c6c4e583bcf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Average best scores: {'accuracy': 0.9961545843791526, 'f2': 0.992554091532768, 'f1': 0.9939792827626656}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import openpyxl\n","model = ['LR', 'LDA', 'SVM', 'KNN', 'XGBoost', 'RF']\n","FS = ['ANOVA', 'MI', 'Pearson', 'Chi2']\n","# Load the existing file\n","book = openpyxl.load_workbook('DWT_Results.xlsx')\n","\n","# Prepare the data to be written\n","data_acc = average_best_scores['accuracy']\n","data_f2 = average_best_scores['f2']\n","data_f1 = average_best_scores['f1']\n","\n","# Get the existing sheets\n","sheet_acc = book['ACC']\n","sheet_f2 = book['F2']\n","sheet_f1 = book['F1']\n","\n","# Calculate the correct row and column numbers\n","row = model.index('RF') + 2  # +2 because Excel index starts from 1 and row 1 contains headers\n","col = FS.index('ANOVA') + 2  # +2 because Excel index starts from 1 and column 1 contains headers\n","\n","# Write to the ACC sheet\n","sheet_acc.cell(row=row, column=col, value=data_acc)\n","\n","# Write to the F2 sheet\n","sheet_f2.cell(row=row, column=col, value=data_f2)\n","\n","# Write to the F1 sheet\n","sheet_f1.cell(row=row, column=col, value=data_f1)\n","\n","# Save and close the Excel file\n","book.save('DWT_Results.xlsx')"],"metadata":{"id":"DlKFOdkwcoE6","executionInfo":{"status":"ok","timestamp":1691180224323,"user_tz":-210,"elapsed":2234,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","import pickle\n","\n","# Specify the directory path\n","dir_path = '/content/drive/MyDrive/Ali Sobhani Thesis/DWT/ANOVA/RF'\n","\n","# Save best_params and all_selected_names to the directory\n","with open(os.path.join(dir_path, 'best_params.pkl'), 'wb') as f:\n","    pickle.dump(best_params, f)\n","with open(os.path.join(dir_path, 'all_selected_names.pkl'), 'wb') as f:\n","    pickle.dump(all_selected_names, f)"],"metadata":{"id":"vzXsOjLacxBZ","executionInfo":{"status":"ok","timestamp":1691180225420,"user_tz":-210,"elapsed":443,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}}},"execution_count":6,"outputs":[]}]}