{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMKw7Ml41nD7IJRGad2CHjG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zhJ-HLd_vaR","executionInfo":{"status":"ok","timestamp":1691131749081,"user_tz":-210,"elapsed":3304,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"51b05268-578b-4b8a-e1f8-45351e96bbd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d375PBZFmCsQ","executionInfo":{"status":"ok","timestamp":1691131749081,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"5cdab2f6-7e74-4b1a-fcc8-e6bbeba4ecc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Ali Sobhani Thesis/TS\n"]}],"source":["cd '/content/drive/MyDrive/Ali Sobhani Thesis/TS'"]},{"cell_type":"code","source":["!pip install scikit-optimize\n","!pip install scikeras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxCaS-ZImgCN","executionInfo":{"status":"ok","timestamp":1691131759357,"user_tz":-210,"elapsed":10281,"user":{"displayName":"Ali Amini","userId":"04196589200907759172"}},"outputId":"f15e8f64-b8d3-4bfe-82bb-7be4f92876f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.9.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.1)\n","Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (23.7.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.22.4)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n","Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.11.0)\n","Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Flatten, Conv2D, DepthwiseConv2D, BatchNormalization, Dense, Activation, Dropout, MaxPooling2D\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.constraints import max_norm\n","from scikeras.wrappers import KerasClassifier\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import fbeta_score\n","from sklearn.model_selection import StratifiedKFold\n","from skopt import BayesSearchCV\n","import numpy as np\n","from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","\n","X = np.load(\"X.npy\", allow_pickle=True)\n","Y = np.load(\"Y.npy\")\n","\n","def create_model4(optimizer='adam', kernel_size=3, dropout=0.5, norm_rate=0.01, F1=16, D=2, activation='relu', kernelsize =2, F2=32):\n","    input1       = Input(shape = (8, 7, 1))\n","    block1       = Conv2D(F1, (1, kernel_size), padding = 'same',\n","                                    input_shape = (1, 8, 7),\n","                                    use_bias = False)(input1)\n","    block1       = BatchNormalization(axis = -1)(block1)\n","    block2       = DepthwiseConv2D((8, 1), use_bias = False,\n","                                      depth_multiplier = D,\n","                                      depthwise_constraint = max_norm(norm_rate))(block1)\n","    block2       = BatchNormalization(axis = -1)(block2)\n","    if activation == 'leakyrelu':\n","        block2   = LeakyReLU()(block2)\n","    else:\n","        block2   = Activation(activation)(block2)\n","    block3       = Dropout(dropout)(block2)\n","    block4       = SeparableConv2D(F2, (1, kernelsize),\n","                                    use_bias = False, padding = 'same')(block3)\n","    block5       = BatchNormalization(axis = -1)(block4)\n","    if activation == 'leakyrelu':\n","        block6   = LeakyReLU()(block5)\n","    else:\n","        block6   = Activation(activation)(block5)\n","    block6       = MaxPooling2D((1, 2))(block6)\n","    flatten      = Flatten(name = 'flatten')(block6)\n","    block7       = Dropout(dropout)(flatten)\n","    dense        = Dense(1, name = 'dense',\n","                          kernel_constraint = max_norm(norm_rate))(block7)\n","    sigmoid      = Activation('sigmoid', name = 'sigmoid')(dense)\n","    model        = Model(inputs=input1, outputs=sigmoid)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=optimizer,\n","                  metrics=['accuracy'])\n","    return model\n","\n","p = {\n","    'optimizer': ['Adam', 'Adamax', 'Nadam'],\n","    'batch_size': [32, 64, 128],\n","    'epochs': [10, 15, 20, 30],\n","    'kernel_size': [2, 3, 4, 5],\n","    'dropout': [0.2, 0.3, 0.4, 0.5],\n","    'norm_rate': [0.01, 0.001, 0.1, 1, 5, 10],\n","    'F1': [16, 32, 64],\n","    'D': [1, 2, 3],\n","    'activation': ['relu', 'elu', 'leakyrelu'],\n","    'F2' : [32,64,128],\n","    'kernelsize' : [2,3]\n","}\n","\n","model = KerasClassifier(model=create_model4, verbose=1, **p)\n","inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\n","outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n","\n","grid = HalvingGridSearchCV(estimator=model, param_grid=p, cv=inner_cv, scoring='accuracy')\n","\n","best_scores = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_params = []\n","\n","for train_idx, test_idx in outer_cv.split(X, Y):\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = Y[train_idx], Y[test_idx]\n","\n","    grid_result = grid.fit(X_train, y_train)\n","\n","    best_params.append(grid_result.best_params_)\n","    #best_scores['accuracy'].append(bayes_result.best_score_)\n","    y_pred = grid.predict(X_test)\n","    f2 = fbeta_score(y_test, y_pred, beta=2)\n","    ################################\n","    from sklearn.metrics import f1_score, accuracy_score\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    ################################\n","    best_scores['f2'].append(f2)\n","    best_scores['f1'].append(f1)\n","    best_scores['accuracy'].append(accuracy)\n","average_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\n","print(f\"Average best scores: {average_best_scores}\")\n"],"metadata":{"id":"71HwEjcO-5I6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import openpyxl\n","model = ['Model1', 'Model2', 'Model3', 'Model4']\n","FS = ['Bayes', 'Random', 'Grid']\n","# Load the existing file\n","book = openpyxl.load_workbook('TS_Results.xlsx')\n","\n","# Prepare the data to be written\n","data_acc = average_best_scores['accuracy']\n","data_f2 = average_best_scores['f2']\n","data_f1 = average_best_scores['f1']\n","\n","# Get the existing sheets\n","sheet_acc = book['ACC']\n","sheet_f2 = book['F2']\n","sheet_f1 = book['F1']\n","\n","# Calculate the correct row and column numbers\n","row = model.index('Model4') + 2  # +2 because Excel index starts from 1 and row 1 contains headers\n","col = FS.index('Grid') + 2  # +2 because Excel index starts from 1 and column 1 contains headers\n","\n","# Write to the ACC sheet\n","sheet_acc.cell(row=row, column=col, value=data_acc)\n","\n","# Write to the F2 sheet\n","sheet_f2.cell(row=row, column=col, value=data_f2)\n","\n","# Write to the F1 sheet\n","sheet_f1.cell(row=row, column=col, value=data_f1)\n","\n","# Save and close the Excel file\n","book.save('TS_Results.xlsx')"],"metadata":{"id":"kIE9Vealv6rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pickle\n","\n","# Specify the directory path\n","dir_path = '/content/drive/MyDrive/Ali Sobhani Thesis/TS/Grid/Model4'\n","\n","# Save best_params and all_selected_names to the directory\n","with open(os.path.join(dir_path, 'best_params.pkl'), 'wb') as f:\n","    pickle.dump(best_params, f)"],"metadata":{"id":"MZTqCmUewW76"},"execution_count":null,"outputs":[]}]}