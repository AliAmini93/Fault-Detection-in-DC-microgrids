{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbseVyir8MZv","executionInfo":{"status":"ok","timestamp":1691571710554,"user_tz":-210,"elapsed":93197,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}},"outputId":"118e0892-255c-4934-f629-c4c0a3a048a5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eH14YbVJF2jJ","executionInfo":{"status":"ok","timestamp":1691571710555,"user_tz":-210,"elapsed":28,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}},"outputId":"7701ca9d-30bc-4ad2-e3a7-ee949e178656"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/10ul3ZK_MC4nZuEgYotLTtUO0eEZB3gue/Ali Sobhani Thesis/Noisy/FFT\n"]}],"source":["cd '/content/drive/MyDrive/Ali Sobhani Thesis/Noisy/FFT'"]},{"cell_type":"code","source":["'''import numpy as np\n","from sklearn.experimental import enable_halving_search_cv  # noqa\n","from sklearn.model_selection import HalvingGridSearchCV, StratifiedKFold\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import fbeta_score\n","from scipy.stats import pearsonr\n","\n","def pearsonr_scorer(X, y):\n","    scores, pvalues = [], []\n","    for column in X.T:\n","        score, pvalue = pearsonr(column, y)\n","        scores.append(abs(score))\n","        pvalues.append(pvalue)\n","    return np.array(scores), np.array(pvalues)\n","\n","feature_selection = SelectKBest(score_func=pearsonr_scorer)\n","feature_names = ['IP1_C1', 'IP1_C2', 'IP1_C3','IP1_C4','IN1_C1', 'IN1_C2', 'IN1_C3', 'IN1_C4',\n","                    'IP2_C1', 'IP2_C2', 'IP2_C3', 'IP2_C4', 'IN2_C1', 'IN2_C2', 'IN2_C3', 'IN2_C4',\n","                    'VP1_C1', 'VP1_C2', 'VP1_C3', 'VP1_C4', 'VN1_C1', 'VN1_C2', 'VN1_C3', 'VN1_C4',\n","                    'VP2_C1', 'VP2_C2', 'VP2_C3', 'VP2_C4', 'VN2_C1', 'VN2_C2', 'VN2_C3', 'VN2_C4']\n","\n","X = np.load('X.npy')\n","Y = np.load('Y.npy')\n","\n","rf = RandomForestClassifier()\n","\n","pipeline = Pipeline([\n","    ('feature_selection', feature_selection),\n","    ('rf', rf)\n","])\n","\n","param_grid = {\n","    'feature_selection__k': list(range(1, X.shape[1] + 1)),\n","    'rf__n_estimators': [10, 50, 100, 200, 500],\n","    'rf__max_depth': [None, 10, 20, 30, 40, 50],\n","    'rf__min_samples_split': [2, 5, 10],\n","    'rf__min_samples_leaf': [1, 2, 4],\n","    'rf__bootstrap': [True, False]\n","}\n","\n","outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n","inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\n","\n","grid_search = HalvingGridSearchCV(pipeline, param_grid, cv=inner_cv, scoring='accuracy')\n","\n","best_scores = {'accuracy': [], 'f2': []}\n","best_params = []\n","all_selected_names = []\n","\n","for train_idx, test_idx in outer_cv.split(X, Y):\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = Y[train_idx], Y[test_idx]\n","\n","    grid_search.fit(X_train, y_train)\n","    best_params.append(grid_search.best_params_)\n","\n","    best_scores['accuracy'].append(grid_search.best_score_)\n","\n","    y_pred = grid_search.predict(X_test)\n","\n","    f2 = fbeta_score(y_test, y_pred, beta=2)\n","    best_scores['f2'].append(f2)\n","\n","    selected_features = grid_search.best_estimator_.named_steps['feature_selection']\n","    selected_names = [feature_names[i] for i in selected_features.get_support(indices=True)]\n","    all_selected_names.append(selected_names)\n","\n","average_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\n","print(f\"Average best scores: {average_best_scores}\")\n","'''"],"metadata":{"id":"WYiiMD9oGuV0","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"9d2a169f-f9cf-45a5-dca5-1b1fd85e82ec","executionInfo":{"status":"ok","timestamp":1691571710556,"user_tz":-210,"elapsed":23,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import numpy as np\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingGridSearchCV, StratifiedKFold\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import fbeta_score\\nfrom scipy.stats import pearsonr\\n\\ndef pearsonr_scorer(X, y):\\n    scores, pvalues = [], []\\n    for column in X.T:\\n        score, pvalue = pearsonr(column, y)\\n        scores.append(abs(score))\\n        pvalues.append(pvalue)\\n    return np.array(scores), np.array(pvalues)\\n\\nfeature_selection = SelectKBest(score_func=pearsonr_scorer)\\nfeature_names = [\\'IP1_C1\\', \\'IP1_C2\\', \\'IP1_C3\\',\\'IP1_C4\\',\\'IN1_C1\\', \\'IN1_C2\\', \\'IN1_C3\\', \\'IN1_C4\\',\\n                    \\'IP2_C1\\', \\'IP2_C2\\', \\'IP2_C3\\', \\'IP2_C4\\', \\'IN2_C1\\', \\'IN2_C2\\', \\'IN2_C3\\', \\'IN2_C4\\',\\n                    \\'VP1_C1\\', \\'VP1_C2\\', \\'VP1_C3\\', \\'VP1_C4\\', \\'VN1_C1\\', \\'VN1_C2\\', \\'VN1_C3\\', \\'VN1_C4\\',\\n                    \\'VP2_C1\\', \\'VP2_C2\\', \\'VP2_C3\\', \\'VP2_C4\\', \\'VN2_C1\\', \\'VN2_C2\\', \\'VN2_C3\\', \\'VN2_C4\\']\\n\\nX = np.load(\\'X.npy\\')\\nY = np.load(\\'Y.npy\\')\\n\\nrf = RandomForestClassifier()\\n\\npipeline = Pipeline([\\n    (\\'feature_selection\\', feature_selection),\\n    (\\'rf\\', rf)\\n])\\n\\nparam_grid = {\\n    \\'feature_selection__k\\': list(range(1, X.shape[1] + 1)),\\n    \\'rf__n_estimators\\': [10, 50, 100, 200, 500],\\n    \\'rf__max_depth\\': [None, 10, 20, 30, 40, 50],\\n    \\'rf__min_samples_split\\': [2, 5, 10],\\n    \\'rf__min_samples_leaf\\': [1, 2, 4],\\n    \\'rf__bootstrap\\': [True, False]\\n}\\n\\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\\ninner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\\n\\ngrid_search = HalvingGridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=\\'accuracy\\')\\n\\nbest_scores = {\\'accuracy\\': [], \\'f2\\': []}\\nbest_params = []\\nall_selected_names = []\\n\\nfor train_idx, test_idx in outer_cv.split(X, Y):\\n    X_train, X_test = X[train_idx], X[test_idx]\\n    y_train, y_test = Y[train_idx], Y[test_idx]\\n\\n    grid_search.fit(X_train, y_train)\\n    best_params.append(grid_search.best_params_)\\n\\n    best_scores[\\'accuracy\\'].append(grid_search.best_score_)\\n\\n    y_pred = grid_search.predict(X_test)\\n\\n    f2 = fbeta_score(y_test, y_pred, beta=2)\\n    best_scores[\\'f2\\'].append(f2)\\n\\n    selected_features = grid_search.best_estimator_.named_steps[\\'feature_selection\\']\\n    selected_names = [feature_names[i] for i in selected_features.get_support(indices=True)]\\n    all_selected_names.append(selected_names)\\n\\naverage_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\\nprint(f\"Average best scores: {average_best_scores}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.experimental import enable_halving_search_cv  # noqa\n","from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import fbeta_score\n","\n","feature_selection = SelectKBest(score_func=mutual_info_classif)\n","feature_names = ['IP1_C1', 'IP1_C2', 'IP1_C3','IP1_C4','IN1_C1', 'IN1_C2', 'IN1_C3', 'IN1_C4',\n","                    'IP2_C1', 'IP2_C2', 'IP2_C3', 'IP2_C4', 'IN2_C1', 'IN2_C2', 'IN2_C3', 'IN2_C4',\n","                    'VP1_C1', 'VP1_C2', 'VP1_C3', 'VP1_C4', 'VN1_C1', 'VN1_C2', 'VN1_C3', 'VN1_C4',\n","                    'VP2_C1', 'VP2_C2', 'VP2_C3', 'VP2_C4', 'VN2_C1', 'VN2_C2', 'VN2_C3', 'VN2_C4']\n","\n","############################\n","X = np.load('X.npy')\n","Y = np.load('Y.npy')\n","X_20 = np.load('X_FFT_SNR_20.npy')\n","Y_20 = np.load('Y_FFT_SNR_20.npy')\n","X_25 = np.load('X_FFT_SNR_25.npy')\n","Y_25 = np.load('Y_FFT_SNR_25.npy')\n","X_30 = np.load('X_FFT_SNR_30.npy')\n","Y_30 = np.load('Y_FFT_SNR_30.npy')\n","X_35 = np.load('X_FFT_SNR_35.npy')\n","Y_35 = np.load('Y_FFT_SNR_35.npy')\n","X_40 = np.load('X_FFT_SNR_40.npy')\n","Y_40 = np.load('Y_FFT_SNR_40.npy')\n","X_45 = np.load('X_FFT_SNR_45.npy')\n","Y_45 = np.load('Y_FFT_SNR_45.npy')\n","###########################\n","\n","\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","pipeline = Pipeline([\n","    ('feature_selection', feature_selection),\n","    ('rf', rf)\n","])\n","\n","param_grid = {\n","    'feature_selection__k': list(range(1, X.shape[1] + 1)),\n","    'rf__n_estimators': [100, 200, 300],\n","    'rf__max_depth': [None, 30, 50],\n","    'rf__min_samples_split': [2, 5, 10],\n","    'rf__min_samples_leaf': [1, 2, 4],\n","    'rf__bootstrap': [True, False]\n","}\n","\n","outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n","inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=13)\n","\n","grid_search = RandomizedSearchCV(pipeline, param_grid, n_iter=100, cv=inner_cv, scoring='accuracy', random_state=13, n_jobs=-1)\n","\n","from sklearn.metrics import f1_score, accuracy_score\n","best_scores = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_20 = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_25 = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_30 = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_35 = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_40 = {'accuracy': [], 'f2': [], 'f1':[]}\n","best_scores_45 = {'accuracy': [], 'f2': [], 'f1':[]}\n","\n","for train_idx, test_idx in outer_cv.split(X, Y):\n","    X_train, X_test, X_test_20, X_test_25, X_test_30, X_test_35, X_test_40, X_test_45 = X[train_idx], X[test_idx], X_20[test_idx], X_25[test_idx], X_30[test_idx], X_35[test_idx], X_40[test_idx], X_45[test_idx]\n","    y_train, y_test, y_test_20, y_test_25, y_test_30, y_test_35, y_test_40, y_test_45 = Y[train_idx], Y[test_idx], Y_20[test_idx], Y_25[test_idx], Y_30[test_idx], Y_35[test_idx], Y_40[test_idx], Y_45[test_idx]\n","\n","    grid_search.fit(X_train, y_train)\n","\n","    y_pred = grid_search.predict(X_test)\n","    y_pred_20 = grid_search.predict(X_test_20)\n","    y_pred_25 = grid_search.predict(X_test_25)\n","    y_pred_30 = grid_search.predict(X_test_30)\n","    y_pred_35 = grid_search.predict(X_test_35)\n","    y_pred_40 = grid_search.predict(X_test_40)\n","    y_pred_45 = grid_search.predict(X_test_45)\n","\n","    f2 = fbeta_score(y_test, y_pred, beta=2)\n","    f2_20 = fbeta_score(y_test_20, y_pred_20, beta=2)\n","    f2_25 = fbeta_score(y_test_25, y_pred_25, beta=2)\n","    f2_30 = fbeta_score(y_test_30, y_pred_30, beta=2)\n","    f2_35 = fbeta_score(y_test_35, y_pred_35, beta=2)\n","    f2_40 = fbeta_score(y_test_40, y_pred_40, beta=2)\n","    f2_45 = fbeta_score(y_test_45, y_pred_45, beta=2)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracy_20 = accuracy_score(y_test_20, y_pred_20)\n","    accuracy_25 = accuracy_score(y_test_25, y_pred_25)\n","    accuracy_30 = accuracy_score(y_test_30, y_pred_30)\n","    accuracy_35 = accuracy_score(y_test_35, y_pred_35)\n","    accuracy_40 = accuracy_score(y_test_40, y_pred_40)\n","    accuracy_45 = accuracy_score(y_test_45, y_pred_45)\n","\n","    f1 = f1_score(y_test, y_pred)\n","    f1_20 = f1_score(y_test_20, y_pred_20)\n","    f1_25 = f1_score(y_test_25, y_pred_25)\n","    f1_30 = f1_score(y_test_30, y_pred_30)\n","    f1_35 = f1_score(y_test_35, y_pred_35)\n","    f1_40 = f1_score(y_test_40, y_pred_40)\n","    f1_45 = f1_score(y_test_45, y_pred_45)\n","\n","    best_scores['f2'].append(f2)\n","    best_scores_20['f2'].append(f2_20)\n","    best_scores_25['f2'].append(f2_25)\n","    best_scores_30['f2'].append(f2_30)\n","    best_scores_35['f2'].append(f2_35)\n","    best_scores_40['f2'].append(f2_40)\n","    best_scores_45['f2'].append(f2_45)\n","\n","    best_scores['f1'].append(f1)\n","    best_scores_20['f1'].append(f1_20)\n","    best_scores_25['f1'].append(f1_25)\n","    best_scores_30['f1'].append(f1_30)\n","    best_scores_35['f1'].append(f1_35)\n","    best_scores_40['f1'].append(f1_40)\n","    best_scores_45['f1'].append(f1_45)\n","\n","    best_scores['accuracy'].append(accuracy)\n","    best_scores_20['accuracy'].append(accuracy_20)\n","    best_scores_25['accuracy'].append(accuracy_25)\n","    best_scores_30['accuracy'].append(accuracy_30)\n","    best_scores_35['accuracy'].append(accuracy_35)\n","    best_scores_40['accuracy'].append(accuracy_40)\n","    best_scores_45['accuracy'].append(accuracy_45)"],"metadata":{"id":"HANE6y7ngc8G","executionInfo":{"status":"ok","timestamp":1691572347403,"user_tz":-210,"elapsed":636868,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["average_best_scores = {scoring: np.mean(scores) for scoring, scores in best_scores.items()}\n","average_best_scores_20 = {scoring: np.mean(scores) for scoring, scores in best_scores_20.items()}\n","average_best_scores_25 = {scoring: np.mean(scores) for scoring, scores in best_scores_25.items()}\n","average_best_scores_30 = {scoring: np.mean(scores) for scoring, scores in best_scores_30.items()}\n","average_best_scores_35 = {scoring: np.mean(scores) for scoring, scores in best_scores_35.items()}\n","average_best_scores_40 = {scoring: np.mean(scores) for scoring, scores in best_scores_40.items()}\n","average_best_scores_45 = {scoring: np.mean(scores) for scoring, scores in best_scores_45.items()}\n","\n","print(f\"Average best scores: {average_best_scores}\")\n","print(f\"Average best scores_20: {average_best_scores_20}\")\n","print(f\"Average best scores_25: {average_best_scores_25}\")\n","print(f\"Average best scores_30: {average_best_scores_30}\")\n","print(f\"Average best scores_35: {average_best_scores_35}\")\n","print(f\"Average best scores_40: {average_best_scores_40}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BXX59qIkr08","executionInfo":{"status":"ok","timestamp":1691572429850,"user_tz":-210,"elapsed":9,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}},"outputId":"aee67b3f-bb6d-4bbe-8104-85b43beb9bd3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Average best scores: {'accuracy': 1.0, 'f2': 1.0, 'f1': 1.0}\n","Average best scores_20: {'accuracy': 0.9546449136276391, 'f2': 0.9279178748504912, 'f1': 0.9295599908991008}\n","Average best scores_25: {'accuracy': 0.9873172892366749, 'f2': 0.979162991609188, 'f1': 0.9802261384587541}\n","Average best scores_30: {'accuracy': 0.9957721836704561, 'f2': 0.9937798837531782, 'f1': 0.9934219511153831}\n","Average best scores_35: {'accuracy': 0.9992315074560757, 'f2': 0.9988072494596552, 'f1': 0.9988059701492537}\n","Average best scores_40: {'accuracy': 0.999616122840691, 'f2': 0.9997607655502392, 'f1': 0.9994029850746269}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import openpyxl\n","model = ['LR', 'LDA', 'SVM', 'KNN', 'XGBoost', 'RF']\n","FS = ['ANOVA', 'MI', 'Pearson', 'Chi2']\n","# Load the existing file\n","book = openpyxl.load_workbook('FFT_Results.xlsx')\n","book_20 = openpyxl.load_workbook('FFT_Results_20.xlsx')\n","book_25 = openpyxl.load_workbook('FFT_Results_25.xlsx')\n","book_30 = openpyxl.load_workbook('FFT_Results_30.xlsx')\n","book_35 = openpyxl.load_workbook('FFT_Results_35.xlsx')\n","book_40 = openpyxl.load_workbook('FFT_Results_40.xlsx')\n","book_45 = openpyxl.load_workbook('FFT_Results_45.xlsx')\n","\n","# Prepare the data to be written\n","data_acc = average_best_scores['accuracy']\n","data_acc_20 = average_best_scores_20['accuracy']\n","data_acc_25 = average_best_scores_25['accuracy']\n","data_acc_30 = average_best_scores_30['accuracy']\n","data_acc_35 = average_best_scores_35['accuracy']\n","data_acc_40 = average_best_scores_40['accuracy']\n","data_acc_45 = average_best_scores_45['accuracy']\n","\n","data_f2 = average_best_scores['f2']\n","data_f2_20 = average_best_scores_20['f2']\n","data_f2_25 = average_best_scores_25['f2']\n","data_f2_30 = average_best_scores_30['f2']\n","data_f2_35 = average_best_scores_35['f2']\n","data_f2_40 = average_best_scores_40['f2']\n","data_f2_45 = average_best_scores_45['f2']\n","\n","data_f1 = average_best_scores['f1']\n","data_f1_20 = average_best_scores_20['f1']\n","data_f1_25 = average_best_scores_25['f1']\n","data_f1_30 = average_best_scores_30['f1']\n","data_f1_35 = average_best_scores_35['f1']\n","data_f1_40 = average_best_scores_40['f1']\n","data_f1_45 = average_best_scores_45['f1']\n","# Get the existing sheets\n","sheet_acc = book['ACC']\n","sheet_acc_20 = book_20['ACC']\n","sheet_acc_25 = book_25['ACC']\n","sheet_acc_30 = book_30['ACC']\n","sheet_acc_35 = book_35['ACC']\n","sheet_acc_40 = book_40['ACC']\n","sheet_acc_45 = book_45['ACC']\n","\n","sheet_f2 = book['F2']\n","sheet_f2_20 = book_20['F2']\n","sheet_f2_25 = book_25['F2']\n","sheet_f2_30 = book_30['F2']\n","sheet_f2_35 = book_35['F2']\n","sheet_f2_40 = book_40['F2']\n","sheet_f2_45 = book_45['F2']\n","\n","sheet_f1 = book['F1']\n","sheet_f1_20 = book_20['F1']\n","sheet_f1_25 = book_25['F1']\n","sheet_f1_30 = book_30['F1']\n","sheet_f1_35 = book_35['F1']\n","sheet_f1_40 = book_40['F1']\n","sheet_f1_45 = book_45['F1']\n","# Calculate the correct row and column numbers\n","row = model.index('RF') + 2  # +2 because Excel index starts from 1 and row 1 contains headers\n","col = FS.index('MI') + 2  # +2 because Excel index starts from 1 and column 1 contains headers\n","\n","# Write to the ACC sheet\n","sheet_acc.cell(row=row, column=col, value=data_acc)\n","sheet_acc_20.cell(row=row, column=col, value=data_acc_20)\n","sheet_acc_25.cell(row=row, column=col, value=data_acc_25)\n","sheet_acc_30.cell(row=row, column=col, value=data_acc_30)\n","sheet_acc_35.cell(row=row, column=col, value=data_acc_35)\n","sheet_acc_40.cell(row=row, column=col, value=data_acc_40)\n","sheet_acc_45.cell(row=row, column=col, value=data_acc_45)\n","\n","# Write to the F2 sheet\n","sheet_f2.cell(row=row, column=col, value=data_f2)\n","sheet_f2_20.cell(row=row, column=col, value=data_f2_20)\n","sheet_f2_25.cell(row=row, column=col, value=data_f2_25)\n","sheet_f2_30.cell(row=row, column=col, value=data_f2_30)\n","sheet_f2_35.cell(row=row, column=col, value=data_f2_35)\n","sheet_f2_40.cell(row=row, column=col, value=data_f2_40)\n","sheet_f2_45.cell(row=row, column=col, value=data_f2_45)\n","\n","# Write to the F1 sheet\n","sheet_f1.cell(row=row, column=col, value=data_f1)\n","sheet_f1_20.cell(row=row, column=col, value=data_f1_20)\n","sheet_f1_25.cell(row=row, column=col, value=data_f1_25)\n","sheet_f1_30.cell(row=row, column=col, value=data_f1_30)\n","sheet_f1_35.cell(row=row, column=col, value=data_f1_35)\n","sheet_f1_40.cell(row=row, column=col, value=data_f1_40)\n","sheet_f1_45.cell(row=row, column=col, value=data_f1_45)\n","# Save and close the Excel file\n","book.save('FFT_Results.xlsx')\n","book_20.save('FFT_Results_20.xlsx')\n","book_25.save('FFT_Results_25.xlsx')\n","book_30.save('FFT_Results_30.xlsx')\n","book_35.save('FFT_Results_35.xlsx')\n","book_40.save('FFT_Results_40.xlsx')\n","book_45.save('FFT_Results_45.xlsx')"],"metadata":{"id":"DlKFOdkwcoE6","executionInfo":{"status":"ok","timestamp":1691573371876,"user_tz":-210,"elapsed":11917,"user":{"displayName":"Armin Amini","userId":"10828257944593691446"}}},"execution_count":8,"outputs":[]}]}